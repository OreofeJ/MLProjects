
# Traffic Sign Detector System

## 1. Introduction
This project develops and evaluates a Traffic Sign Detector System using various Convolutional Neural Network (CNN) architectures. The goal is to accurately classify traffic signs from images, which is crucial for autonomous driving and advanced driver-assistance systems. We explore a baseline CNN, an augmented CNN, and a pre-trained MobileNetV2 model to identify the most effective approach.

## 2. Dataset Description
The system utilizes the **German Traffic Sign Recognition Benchmark (GTSRB)** dataset. This dataset contains over 50,000 images of traffic signs belonging to 43 different classes. Each image is provided with its corresponding `ClassId` and image path, along with bounding box information (`Roi.X1`, `Roi.Y1`, `Roi.X2`, `Roi.Y2`), `Width`, and `Height`.

-   **Training Data (`Train.csv`):** Contains image paths and labels for training.
-   **Test Data (`Test.csv`):** Contains image paths for final evaluation.
-   **Meta Data (`Meta.csv`):** Provides additional information about the classes (not directly used for training in this setup).

**Number of classes:** 43 unique traffic sign classes.

## 3. Identified Dataset Issues
During the Exploratory Data Analysis (EDA), several potential issues were identified that could impact model performance:

1.  **Class Imbalance:** Some traffic sign classes have significantly more samples than others, which could lead to biased learning towards over-represented classes.
2.  **Varying Image Sizes:** Images within the dataset have different resolutions, requiring resizing to a fixed input shape for CNNs.
3.  **Lighting & Weather Variations:** Images captured under real-world conditions exhibit inconsistent brightness and contrast, posing a challenge for robust feature extraction.
4.  **Motion Blur & Occlusion:** Some traffic signs are partially hidden or blurred, making them difficult to classify.
5.  **Visual Similarity Between Classes:** Certain traffic signs, especially speed-limit signs (e.g., 30, 50, 60 km/h), are visually very similar, which can make differentiation harder for the model.

## 4. Preprocessing Steps

### 4.1 Image Resizing and Normalization
All images were resized to a uniform `(32, 32)` pixels to serve as a fixed input for the CNN models. Additionally, pixel values were normalized by dividing by 255.0 to scale them to the range [0, 1]. This helps in faster convergence and better performance during training.

### 4.2 Data Split
The preprocessed data was split into training, validation, and test sets using a stratified approach to maintain class distribution:
-   **Training Set:** 70% of the data.
-   **Validation Set:** 15% of the data.
-   **Test Set:** 15% of the data.

### 4.3 Data Augmentation
To address issues like class imbalance and to improve the model's generalization capabilities, data augmentation was applied to the training set. The `ImageDataGenerator` from Keras was used with the following transformations:
-   `rotation_range=15`
-   `width_shift_range=0.1`
-   `height_shift_range=0.1`
-   `zoom_range=0.1`
-   `shear_range=0.1`

## 5. Model Architectures

### 5.1 Baseline CNN Model
Our baseline model is a sequential CNN consisting of:
-   Two `Conv2D` layers with `ReLU` activation, followed by `MaxPooling2D` and `BatchNormalization`.
    -   First `Conv2D`: 32 filters, `(3,3)` kernel, `input_shape=(32,32,3)`
    -   Second `Conv2D`: 64 filters, `(3,3)` kernel
-   A `Flatten` layer to convert the 2D feature maps into a 1D vector.
-   A `Dense` hidden layer with 128 units and `ReLU` activation, with `Dropout(0.5)` for regularization.
-   A final `Dense` output layer with 43 units (one for each class) and `softmax` activation.

The model was compiled with `Adam` optimizer (learning rate 0.001) and `sparse_categorical_crossentropy` loss.

### 5.2 Augmented CNN Model
This model uses the same architecture as the Baseline CNN but is trained with the augmented training data generated by `ImageDataGenerator`. This helps the model see a wider variety of images, making it more robust to variations.

### 5.3 MobileNetV2 Model (Pre-trained)
We also experimented with a pre-trained MobileNetV2 model. This model was used as a feature extractor (`include_top=False`) with its weights frozen (`trainable=False`). A custom classification head was added on top:
-   The frozen `base_model` (MobileNetV2 with `input_shape=(32,32,3)`).
-   A `Flatten` layer.
-   A `Dense` hidden layer with 128 units and `ReLU` activation, with `Dropout(0.5)`.
-   A final `Dense` output layer with 43 units and `softmax` activation.

The model was compiled with `Adam` optimizer (learning rate 0.0001) and `sparse_categorical_crossentropy` loss.

## 6. Model Performance Analysis

Here's a comparison of the test accuracies for the three models:

| Model           | Test Accuracy |
| :-------------- | :------------ |
| Baseline CNN    | 0.9888        |
| Augmented CNN   | 0.9881        |
| MobileNetV2     | 0.2567        |

### 6.1 Baseline CNN Performance
-   The **Baseline CNN** achieved an impressive test accuracy of **98.88%** after 10 epochs. This indicates that a relatively simple CNN architecture can perform very well on the GTSRB dataset, even without data augmentation.
-   The `classification_report` shows high precision, recall, and f1-scores across most classes, suggesting good generalization.

### 6.2 Augmented CNN Performance
-   The **Augmented CNN** also performed very well, with a test accuracy of **98.81%** after 15 epochs. While slightly lower than the baseline in this particular run (which can happen due to randomness in training and validation split), data augmentation is generally expected to improve robustness and prevent overfitting, especially with larger datasets or more complex models. The similar accuracy here suggests the baseline was already performing near optimal for this specific data split or that 15 epochs were sufficient.
-   The training process with augmentation took longer per epoch due to the on-the-fly image transformations.

### 6.3 MobileNetV2 Performance Analysis
-   The **MobileNetV2** model performed significantly worse, achieving a test accuracy of only **25.67%** after 20 epochs. This poor performance is a critical observation and can be attributed to several factors, primarily related to its design and how it was used in this context:
    1.  **Input Shape Mismatch/Incompatibility**: MobileNetV2 is typically pre-trained on ImageNet, which consists of much larger images (e.g., 224x224 pixels). When using `input_shape=(32,32,3)` during initialization, a warning was issued by Keras: `UserWarning: input_shape is undefined or non-square, or rows is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.` This indicates that the pre-trained weights, optimized for 224x224 images, were loaded onto a model expecting 32x32 inputs. This size discrepancy means that the learned features from ImageNet might not be effectively transferable to such small input images, as the spatial hierarchies of features (e.g., edges, textures, shapes) might be lost or become irrelevant at a 32x32 resolution.
    2.  **Feature Extraction Effectiveness**: At a very small resolution of 32x32, the rich, high-level features that MobileNetV2 is designed to extract (like objects and parts of objects in ImageNet) might not be present or discernable in traffic sign images. The complex architecture of MobileNetV2 might be overkill for such low-resolution inputs, and its deep layers might struggle to find meaningful patterns.
    3.  **Transfer Learning Limitations**: While transfer learning is powerful, its effectiveness depends on the similarity between the source task (ImageNet classification) and the target task (traffic sign classification), as well as the input data characteristics. The significant difference in image resolution and potentially the complexity of features at that scale likely hindered the transferability of MobileNetV2's pre-trained knowledge.
    4.  **Freezing Base Layers**: By freezing the `base_model.trainable = False`, we prevented the MobileNetV2 layers from learning features specific to the 32x32 traffic sign images. If fine-tuning (unfreezing some layers and training with a very small learning rate) were applied, it might have yielded better results, allowing the model to adapt its filters to the smaller input size and specific traffic sign features.

## 7. Conclusion
Both the Baseline CNN and the Augmented CNN models demonstrated excellent performance in classifying traffic signs from the GTSRB dataset, achieving accuracies close to 99%. This indicates that for this particular task and given the input image size of 32x32, a custom-built CNN is highly effective.

The MobileNetV2 model, despite being a powerful pre-trained architecture, performed poorly due to the significant mismatch between its pre-trained input size (224x224) and the target input size (32x32). This highlights a critical consideration in transfer learning: ensuring that the input data characteristics are reasonably compatible with the pre-trained model's original training conditions.
